{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/Evilown/anaconda3/lib/python36.zip',\n",
       " '/Users/Evilown/anaconda3/lib/python3.6',\n",
       " '/Users/Evilown/anaconda3/lib/python3.6/lib-dynload',\n",
       " '/Users/Evilown/anaconda3/lib/python3.6/site-packages',\n",
       " '/Users/Evilown/anaconda3/lib/python3.6/site-packages/aeosa',\n",
       " '/Users/Evilown/anaconda3/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/Users/Evilown/.ipython',\n",
       " '..']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CWD_PATH = os.getcwd()\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for \n",
    "# the object detection.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "PATH_TO_CKPT = os.path.join(CWD_PATH, 'Tensorflow_detection_model_zoo', \n",
    "                            MODEL_NAME, 'frozen_inference_graph.pb')\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join(CWD_PATH, 'data', 'mscoco_label_map.pbtxt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 90\n",
    "\n",
    "# Loading label map\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, \n",
    "                                                            max_num_classes=NUM_CLASSES,\n",
    "                                                            use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape( (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def save_nparray_into_img(array, name = 'saved_boxed_image'):\n",
    "    im = Image.fromarray(array)\n",
    "    SAVE_PATH = os.path.join(CWD_PATH, 'test_images')\n",
    "    im.save(\"test_images/saved_boxed_file.jpeg\")\n",
    "    print('Image saved to {}'.format(SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_objects(image_np, sess, detection_graph):\n",
    "    ''' Input: Original_test_image, tf.Session(), the_frozen_TF_graph\n",
    "        Output: Labelled_and_Boxed_image '''\n",
    "    \n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0) #[None,None,3] -> [1,None,None,3]\n",
    "    \n",
    "    # Definite input and output Tensors for detection_graph\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "    # Each box represents a part of the image where a particular object was detected.\n",
    "    detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "    # Each score represent how level of confidence for each of the objects.\n",
    "    # Score is shown on the result image, together with the class label.\n",
    "    detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "    # Actual detection.\n",
    "    (boxes, scores, classes, num_detections) = sess.run(\n",
    "        [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "        feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32),\n",
    "        np.squeeze(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "    return image_np, boxes, scores, classes, num_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    \n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First test on images\n",
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg   &   image2.jpg\n",
    "PATH_TO_TEST_IMAGES_DIR = 'test_images'\n",
    "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, \n",
    "                                  'image{}.jpg'.format(i)) for i in range(0, 1) ]\n",
    "\n",
    "# Size, in inches, of the output images. ->(how large the result plt is)\n",
    "IMAGE_SIZE = (12, 8)\n",
    "a = TEST_IMAGE_PATHS[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size: (1352, 900), image_np size: (900, 1352, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Take a look at the images\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    image = Image.open(image_path)\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    #plt.imshow(image_np)\n",
    "    print('image size: {}, image_np size: {}'.format(image.size, image_np.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1352, 3)\n",
      "Image saved to /Users/Evilown/Desktop/object_detection/test_images\n"
     ]
    }
   ],
   "source": [
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        \n",
    "        for image_path in TEST_IMAGE_PATHS:\n",
    "            image = Image.open(image_path)\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            \n",
    "            image_boxed, boxes, scores, classes, num_detection= detect_objects(image_np, sess, detection_graph)\n",
    "            print(image_boxed.shape)\n",
    "            #plt.figure(figsize=IMAGE_SIZE)\n",
    "            #plt.imshow(image_boxed)\n",
    "            \n",
    "            save_nparray_into_img(image_boxed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dont plot but output results greater than thres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def box_N_class_output(image,\n",
    "                       boxes,\n",
    "                       classes,\n",
    "                       scores,\n",
    "                       category_index,\n",
    "                       max_boxes_to_return=20,\n",
    "                       min_score_thresh=.5,\n",
    "                       agnostic_mode=False,\n",
    "                       line_thickness=4):\n",
    "    \n",
    "    class_map = []\n",
    "    box_map = []\n",
    "    score_map = []\n",
    "\n",
    "    if not max_boxes_to_return:\n",
    "        max_boxes_to_return = boxes.shape[0]\n",
    "        \n",
    "    for i in range(min(max_boxes_to_return, boxes.shape[0])):\n",
    "        if scores is None or scores[i] > min_score_thresh:\n",
    "            box = boxes[i].tolist()\n",
    "            box_map.append(box)\n",
    "\n",
    "       \n",
    "            if classes[i] in category_index.keys():\n",
    "                class_name = category_index[classes[i]]['name']\n",
    "            else:\n",
    "                class_name = 'N/A'\n",
    "                \n",
    "            class_map.append(class_name)\n",
    "            score_map.append(scores[i])    \n",
    "                    \n",
    "    num_detected = len(score_map)\n",
    "    return box_map,score_map,class_map,num_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_objects(image_np, sess, detection_graph):\n",
    "    ''' Input: Original_test_image, tf.Session(), the_frozen_TF_graph\n",
    "        Output: Labelled_and_Boxed_image '''\n",
    "    \n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0) #[None,None,3] -> [1,None,None,3]\n",
    "    \n",
    "    # Definite input and output Tensors for detection_graph\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "    # Each box represents a part of the image where a particular object was detected.\n",
    "    detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "    # Each score represent how level of confidence for each of the objects.\n",
    "    # Score is shown on the result image, together with the class label.\n",
    "    detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "    # Actual detection.\n",
    "    (boxes, scores, classes, num_detections) = sess.run(\n",
    "        [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "        feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "    # Selecting high possibility results\n",
    "    (boxes, scores, classes, num_detections) = box_N_class_output(\n",
    "        image_np,\n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32),\n",
    "        np.squeeze(scores),\n",
    "        category_index,\n",
    "        max_boxes_to_return=20)\n",
    "    return image_np, boxes, scores, classes, num_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1352, 3)\n"
     ]
    }
   ],
   "source": [
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        \n",
    "        for image_path in TEST_IMAGE_PATHS:\n",
    "            image = Image.open(image_path)\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            \n",
    "            image_boxed, boxes, scores, classes, num_detection= detect_objects(image_np, sess, detection_graph)\n",
    "            print(image_boxed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5538768172264099,\n",
       "  0.39422380924224854,\n",
       "  0.5931246876716614,\n",
       "  0.40913766622543335,\n",
       "  'person'],\n",
       " [0.3829464316368103,\n",
       "  0.34582412242889404,\n",
       "  0.40220093727111816,\n",
       "  0.3590298891067505,\n",
       "  'kite'],\n",
       " [0.5741666555404663,\n",
       "  0.057666998356580734,\n",
       "  0.6233518123626709,\n",
       "  0.0747537910938263,\n",
       "  'person'],\n",
       " [0.07991442084312439,\n",
       "  0.43740910291671753,\n",
       "  0.16590245068073273,\n",
       "  0.5006028413772583,\n",
       "  'kite'],\n",
       " [0.26564282178878784,\n",
       "  0.2011229395866394,\n",
       "  0.3075351119041443,\n",
       "  0.22309386730194092,\n",
       "  'kite'],\n",
       " [0.6833807826042175,\n",
       "  0.0784299373626709,\n",
       "  0.8405881524085999,\n",
       "  0.11782577633857727,\n",
       "  'person'],\n",
       " [0.38510024547576904,\n",
       "  0.43172216415405273,\n",
       "  0.40073245763778687,\n",
       "  0.44773054122924805,\n",
       "  'kite'],\n",
       " [0.7606196403503418,\n",
       "  0.15739655494689941,\n",
       "  0.9369254112243652,\n",
       "  0.20186904072761536,\n",
       "  'person'],\n",
       " [0.5428125262260437,\n",
       "  0.25604742765426636,\n",
       "  0.5623460412025452,\n",
       "  0.26740866899490356,\n",
       "  'person'],\n",
       " [0.5870811343193054,\n",
       "  0.026993142440915108,\n",
       "  0.6204380393028259,\n",
       "  0.04133802652359009,\n",
       "  'person']]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(classes)):\n",
    "    boxes[i].append(classes[i])\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"[[0.5538768172264099, 0.39422380924224854, 0.5931246876716614, 0.40913766622543335, 'person'], [0.3829464316368103, 0.34582412242889404, 0.40220093727111816, 0.3590298891067505, 'kite'], [0.5741666555404663, 0.057666998356580734, 0.6233518123626709, 0.0747537910938263, 'person'], [0.07991442084312439, 0.43740910291671753, 0.16590245068073273, 0.5006028413772583, 'kite'], [0.26564282178878784, 0.2011229395866394, 0.3075351119041443, 0.22309386730194092, 'kite'], [0.6833807826042175, 0.0784299373626709, 0.8405881524085999, 0.11782577633857727, 'person'], [0.38510024547576904, 0.43172216415405273, 0.40073245763778687, 0.44773054122924805, 'kite'], [0.7606196403503418, 0.15739655494689941, 0.9369254112243652, 0.20186904072761536, 'person'], [0.5428125262260437, 0.25604742765426636, 0.5623460412025452, 0.26740866899490356, 'person'], [0.5870811343193054, 0.026993142440915108, 0.6204380393028259, 0.04133802652359009, 'person']]\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = [[0.5538768172264099,\n",
    "  0.39422380924224854,\n",
    "  0.5931246876716614,\n",
    "  0.40913766622543335,\n",
    "  'person'],\n",
    " [0.3829464316368103,\n",
    "  0.34582412242889404,\n",
    "  0.40220093727111816,\n",
    "  0.3590298891067505,\n",
    "  'kite'],\n",
    " [0.5741666555404663,\n",
    "  0.057666998356580734,\n",
    "  0.6233518123626709,\n",
    "  0.0747537910938263,\n",
    "  'person'],\n",
    " [0.07991442084312439,\n",
    "  0.43740910291671753,\n",
    "  0.16590245068073273,\n",
    "  0.5006028413772583,\n",
    "  'kite'],\n",
    " [0.26564282178878784,\n",
    "  0.2011229395866394,\n",
    "  0.3075351119041443,\n",
    "  0.22309386730194092,\n",
    "  'kite'],\n",
    " [0.6833807826042175,\n",
    "  0.0784299373626709,\n",
    "  0.8405881524085999,\n",
    "  0.11782577633857727,\n",
    "  'person'],\n",
    " [0.38510024547576904,\n",
    "  0.43172216415405273,\n",
    "  0.40073245763778687,\n",
    "  0.44773054122924805,\n",
    "  'kite'],\n",
    " [0.7606196403503418,\n",
    "  0.15739655494689941,\n",
    "  0.9369254112243652,\n",
    "  0.20186904072761536,\n",
    "  'person'],\n",
    " [0.5428125262260437,\n",
    "  0.25604742765426636,\n",
    "  0.5623460412025452,\n",
    "  0.26740866899490356,\n",
    "  'person'],\n",
    " [0.5870811343193054,\n",
    "  0.026993142440915108,\n",
    "  0.6204380393028259,\n",
    "  0.04133802652359009,\n",
    "  'person']]\n",
    "str.encode(str(boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"boxes_with_classes.csv\", \"w\") as f:  \n",
    "    w = csv.writer(f)\n",
    "    w.writerows(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## On Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "              you should return the final output (image with lines are drawn on lanes)'''\n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session(graph=detection_graph) as sess:\n",
    "            image_boxed = detect_objects(image, sess, detection_graph)\n",
    "            return image_boxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video video1_out.mp4\n",
      "[MoviePy] Writing video video1_out.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 60/61 [02:49<00:02,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: video1_out.mp4 \n",
      "\n",
      "CPU times: user 2min 55s, sys: 24 s, total: 3min 19s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "white_output = 'video1_out.mp4'\n",
    "clip1 = VideoFileClip(\"video1.mp4\").subclip(0,2)\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!s\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"760\" height=\"340\" controls>\n",
       "  <source src=\"video1_out.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"760\" height=\"340\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video cars_out.mp4\n",
      "[MoviePy] Writing video cars_out.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:44<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: cars_out.mp4 \n",
      "\n",
      "CPU times: user 1min 25s, sys: 4.6 s, total: 1min 30s\n",
      "Wall time: 44.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "white_output1 = 'cars_out.mp4'\n",
    "clip1 = VideoFileClip(\"cars.mp4\").subclip(0,2)\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!s\n",
    "%time white_clip.write_videofile(white_output1, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"cars_out.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video fruits1_out.mp4\n",
      "[MoviePy] Writing video fruits1_out.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:21<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: fruits1_out.mp4 \n",
      "\n",
      "CPU times: user 41.9 s, sys: 1.65 s, total: 43.6 s\n",
      "Wall time: 21.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "white_output2 = 'fruits1_out.mp4'\n",
    "clip2 = VideoFileClip(\"fruits1.mp4\").subclip(0,1)\n",
    "white_clip = clip2.fl_image(process_image) #NOTE: this function expects color images!!s\n",
    "%time white_clip.write_videofile(white_output2, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"fruits1_out.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video dog_out.mp4\n",
      "[MoviePy] Writing video dog_out.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:43<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: dog_out.mp4 \n",
      "\n",
      "CPU times: user 1min 25s, sys: 3.58 s, total: 1min 29s\n",
      "Wall time: 43.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "white_output3 = 'dog_out.mp4'\n",
    "clip3 = VideoFileClip(\"dog.mp4\").subclip(12,14)\n",
    "white_clip = clip3.fl_image(process_image) #NOTE: this function expects color images!!s\n",
    "%time white_clip.write_videofile(white_output3, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"dog_out.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video my_concatenation.mp4\n",
      "[MoviePy] Writing video my_concatenation.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 189.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: my_concatenation.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge videos\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "clip1 = VideoFileClip(\"cars_out.mp4\")\n",
    "clip2 = VideoFileClip(\"fruits1_out.mp4\")\n",
    "clip3 = VideoFileClip(\"dog_out.mp4\")\n",
    "final_clip = concatenate_videoclips([clip1,clip2,clip3], method=\"compose\")\n",
    "final_clip.write_videofile(\"my_concatenation.mp4\",bitrate=\"5000k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MoviePy] Building file final.gif with imageio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:11<00:00, 12.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "clip = VideoFileClip(\"my_concatenation.mp4\")\n",
    "clip.write_gif(\"final.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
